# Machine Learning training model for sentiment analysis

#.libPaths("U:/R/library")

#library(twitteR)
#library(ROAuth)
#library(tidyverse)
library(purrrlyr)
library(text2vec)
library(caret)
library(glmnet)
#library(ggrepel)

# source: https://analyzecore.com/2017/02/08/twitter-sentiment-analysis-doc2vec/

### loading and preprocessing a training set of tweets
# function for converting some symbols
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")

##### loading classified tweets ######
# source: http://help.sentiment140.com/for-students/
# 0 - the polarity of the tweet (0 = negative, 4 = positive)
# 1 - the id of the tweet
# 2 - the date of the tweet
# 3 - the query. If there is no query, then this value is NO_QUERY.
# 4 - the user that tweeted
# 5 - the text of the tweet

tweets_classified <- read_csv("DataFiles/training.1600000.processed.noemoticon.csv",
                              col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%
  # converting some symbols
  dmap_at('text', conv_fun) %>%
  # replacing class values
  mutate(sentiment = ifelse(sentiment == 0, 0, 1))

# there are some tweets with NA ids that we replace with dummies
tweets_classified_na <- tweets_classified %>%
  filter(is.na(id) == TRUE) %>%
  mutate(id = c(1:n()))
tweets_classified <- tweets_classified %>%
  filter(!is.na(id)) %>%
  rbind(., tweets_classified_na)

# data splitting on train and test
set.seed(2340)
trainIndex <- createDataPartition(tweets_classified$sentiment, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)
tweets_train <- tweets_classified[trainIndex, ]
tweets_test <- tweets_classified[-trainIndex, ]

##### Vectorization #####
# define preprocessing function and tokenization function
prep_fun <- tolower
tok_fun <- word_tokenizer

it_train <- itoken(tweets_train$text, 
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun,
                   ids = tweets_train$id,
                   progressbar = TRUE)
it_test <- itoken(tweets_test$text, 
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun,
                  ids = tweets_test$id,
                  progressbar = TRUE)

# creating vocabulary and document-term matrix
vocab <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dtm_test <- create_dtm(it_test, vectorizer)
# define tf-idf model
tfidf <- TfIdf$new()
# fit the model to the train data and transform it with the fitted model
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
dtm_test_tfidf <- fit_transform(dtm_test, tfidf)

# train the model
t1 <- Sys.time()
glmnet_classifier <- cv.glmnet(x = dtm_train_tfidf,
                               y = tweets_train[['sentiment']], 
                               family = 'binomial', 
                               # L1 penalty
                               alpha = 1,
                               # interested in the area under ROC curve
                               type.measure = "auc",
                               # 5-fold cross-validation
                               nfolds = 5,
                               # high value is less accurate, but has faster training
                               thresh = 1e-3,
                               # again lower number of iterations for faster training
                               maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'mins'))

plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))

preds <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[ ,1]
auc(as.numeric(tweets_test$sentiment), preds)

# save the model for future using
saveRDS(glmnet_classifier, 'glmnet_classifier.RDS')
#######################################################

# read in MCC Tweets and apply sentiment using the machine learning algorithm

MCCTweets <- read_csv("DataFiles/Nov_29_09_47_06_2017_GMT_@ManCityCouncil_TwitterData.csv")

# converting some symbols
MCCTweets <- MCCTweets %>%
  dmap_at('text', conv_fun) %>%
  filter(!str_detect(text, "^RT"))

# preprocessing and tokenization
it_tweets <- itoken(MCCTweets$text,
                    preprocessor = prep_fun,
                    tokenizer = tok_fun,
                    ids = MCCTweets$id,
                    progressbar = TRUE)

# creating vocabulary and document-term matrix
dtm_tweets <- create_dtm(it_tweets, vectorizer)

# transforming data with tf-idf
dtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)

# loading classification model
glmnet_classifier <- readRDS('glmnet_classifier.RDS')

# predict probabilities of positiveness
preds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]

# adding rates to initial dataset
MCCTweets$sentiment <- preds_tweets

# word analysis of positive or negative tweets
# source("TrainingModel.R")
# source("MCCTweetsData.R")


.libPaths("U:/R/library")
source("U:/R/Code/ggplottheme.R")

library(wordcloud2)
library(stringr)
library(tidytext)
library(igraph)
library(ggraph)


TweetText <- MCCTweets

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|\\<U[^\\>]*\\>+|&amp;|&lt;|&gt;|RT|https|"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

CleanTweets <- TweetText %>% 
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  mutate(text = str_replace_all(text, "(@)([^ ]+)", "")) %>%
  mutate(text = str_replace_all(text, "(#)","")) %>%
  mutate(text = str_replace_all(text," "," ")) %>%
  filter(str_length(text)>8)

# Negative sentiment <0.3, positive >0.65, neutral is in-between)
SentWords <- CleanTweets %>%
  filter(sentiment > 0.65) %>%
  arrange(sentiment)

# Negative <0.3, Positive >0.65, neutral in between

#NegativeSentiment <- AllWords %>%
#  filter(sentiment < 0.3) %>%
#  arrange(sentiment)

AllWords <- SentWords %>%
  unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
  filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))

wordcount <- AllWords %>%
  mutate(word = str_replace_all(AllWords$word,"#[a-z,A-Z]*",""))

wordcount <- wordcount %>%
  filter(!str_detect(word, "^@")) %>%
  filter(!str_detect(word, "^00")) %>%
  filter(word != "") %>%
  filter(word != "ed") %>%
  filter(word != "fe0f") %>%
  count(word, sort = TRUE)

wordcount %>%
  mutate(word = reorder(word, n)) %>%
  top_n(20) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity", color = "#733151", fill = "#733151") +
  ggtitle("Top 10 Words From Negative Tweets") +
  theme_LeoCorp() +
  coord_flip()

wordcloud2(wordcount)


Bigrams <- SentWords %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = FALSE)

Bigrams <- Bigrams %>%
  filter(!str_detect(word1, "^@"),
         !str_detect(word2, "^@"))

bigram_graph <- Bigrams %>%
  filter(n>=1) %>%
  graph_from_data_frame()

#netwok of links between words
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, "inches")) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

#Bigrams wordcloud needs to combine words and add counts
#wordcloud2(Bigrams)

# analysis of accounts by sentiment

# repeat for tweets directly to MCC

toMCCCleanTweets <- MCCTweets %>%
  filter(str_detect(text, "^(@ManCityCouncil)")) %>%
  filter(!str_detect(text, "^RT")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  mutate(text = str_replace_all(text, "(@)([^ ]+)", "")) %>%
  mutate(text = str_replace_all(text, "(#)","")) %>%
  mutate(text = gsub("^\\s+|\\s+$", "", text))


Accounts <- toMCCCleanTweets %>%
  group_by(screenName) %>%
  summarise(
    n = n(),
    AveSent = mean(sentiment)
  ) %>%
  arrange(AveSent)


Accounts %>%
  mutate(screenName = reorder(screenName, AveSent)) %>%
  top_n(25) %>%
  ggplot(aes(screenName, AveSent)) +
  geom_bar(stat = "identity", color = "#733151", fill = "#733151") +
  ggtitle("Top 25 Most Positive Tweeters") +
  theme_LeoCorp() +
  coord_flip()


NegativeSentiment <- toMCCCleanTweets %>%
  filter(sentiment < 0.3) %>%
  arrange(sentiment)

NeutralSentiment <- toMCCCleanTweets %>%
  filter(sentiment > 0.3 & sentiment < 0.65) %>%
  arrange(sentiment)

PositiveSentiment <- toMCCCleanTweets %>%
  filter(sentiment > 0.65) %>%
  arrange(sentiment)


NegTweetAcc <- NegativeSentiment %>%
  group_by(screenName) %>%
  summarise(
    NegTweets = n(),
    AveNegSent = mean(sentiment)
  ) %>%
  arrange(AveNegSent)

PosTweetAcc <- PositiveSentiment %>%
  group_by(screenName) %>%
  summarise(
    PosTweets = n(),
    AvePosSent = mean(sentiment)
  ) %>%
  arrange(AvePosSent)

NeutTweetAcc <- NeutralSentiment %>%
  group_by(screenName) %>%
  summarise(
    NeutTweets= n(),
    AveNeutSent = mean(sentiment)
  ) %>%
  arrange(AveNeutSent)

Accounts <- Accounts %>%
  left_join(NegTweetAcc, by = "screenName") %>%
  left_join(PosTweetAcc, by = "screenName") %>%
  left_join(NeutTweetAcc, by = "screenName")

Accounts %>%
#  filter(n>1) %>%
  ggplot() +
  geom_point(aes(reorder(screenName, AveSent), AveSent), size = 0.5, color = "Black") +
  geom_point(aes(screenName, AveNegSent, size = NegTweets), color = "red") +
  geom_point(aes(screenName, AveNeutSent, size = NeutTweets), color = "blue") +
  geom_point(aes(screenName, AvePosSent, size = PosTweets), color = "green") +
  theme_LeoCorp() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("Twitter Account Screen Name") +
  ylab("Average Sentiment (0=Negative, 1=Positive)") +
  ggtitle("Average Sentiment and Number of Tweets by User")


# sentiment graph by date

# color palette
cols <- c("#ce472e", "#f05336", "#ffd73e", "#eec73a", "#4ab04a")

set.seed(932)
samp_ind <- sample(c(1:nrow(MCCTweets)), nrow(MCCTweets) * 0.1) # 10% for labeling

# plotting
ggplot(MCCTweets, aes(x = created, y = sentiment, color = sentiment)) +
  theme_minimal() +
  scale_color_gradientn(colors = cols, limits = c(0, 1),
                        breaks = seq(0, 1, by = 1/4),
                        labels = c("0", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),
                        guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +
  geom_point(aes(color = sentiment), alpha = 0.8) +
  geom_hline(yintercept = 0.65, color = "#4ab04a", size = 1.5, alpha = 0.6, linetype = "longdash") +
  geom_hline(yintercept = 0.35, color = "#f05336", size = 1.5, alpha = 0.6, linetype = "longdash") +
  geom_smooth(size = 1.2, alpha = 0.2) +
#  geom_label_repel(data = MCCTweets[samp_ind, ],
#                   aes(label = round(sentiment, 2)),
#                   fontface = 'bold',
#                   size = 2.5,
#                   max.iter = 200) +
  theme(legend.position = 'bottom',
        legend.direction = "horizontal",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 20, face = "bold", vjust = 2, color = 'black', lineheight = 0.8),
        axis.title.x = element_text(size = 16),
        axis.title.y = element_text(size = 16),
        axis.text.y = element_text(size = 8, face = "bold", color = 'black'),
        axis.text.x = element_text(size = 8, face = "bold", color = 'black')) +
  ggtitle("Tweets Sentiment rate (probability of positiveness)")
  
  
